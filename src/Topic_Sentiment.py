from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import numpy as np
from datetime import datetime
import json
import os
import pickle
from bertopic import BERTopic
from sklearn.cluster import KMeans
from umap import UMAP
from hdbscan import HDBSCAN
from transformers import pipeline
import torch
import pandas as pd

print("="*80)
print("üöÄ BERTopic + Sentiment Processor for Docker")
print("="*80)

# Kh·ªüi t·∫°o Spark Session v·ªõi config ph√π h·ª£p cho Docker
spark = SparkSession.builder \
    .appName("BERTopicSentimentProcessor") \
    .config("spark.jars.packages", 
            "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,"
            "org.mongodb.spark:mongo-spark-connector_2.12:10.2.0,"
            "org.elasticsearch:elasticsearch-spark-30_2.12:8.8.0") \
    .config("spark.mongodb.output.uri", "mongodb://mongo-v4:27017/news_db.doc_topics") \
    .config("spark.es.nodes", "elasticsearch-v4") \
    .config("spark.es.port", "9200") \
    .config("spark.es.resource", "news_enriched") \
    .config("spark.es.nodes.wan.only", "false") \
    .config("spark.broadcast.compress", "true") \
    .config("spark.shuffle.compress", "true") \
    .config("spark.sql.streaming.stopGracefullyOnShutdown", "true") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.maxResultSize", "2g") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# C·∫•u h√¨nh paths v√† topics
KAFKA_BOOTSTRAP_SERVERS = "kafka-v4:29092"
KAFKA_INPUT_TOPIC = "processed_data"
KAFKA_OUTPUT_TOPIC = "enriched_news"
MODEL_PATH = "/opt/spark/work-dir/models/bertopic_model.pkl"
CHECKPOINT_PATH = "/opt/spark/work-dir/checkpoints/topic_sentiment"
NUM_TOPICS = 20
MIN_TOPIC_SIZE = 5

print(f"üìÅ Model path: {MODEL_PATH}")
print(f"üìÅ Checkpoint path: {CHECKPOINT_PATH}")
print(f"üîó Kafka: {KAFKA_BOOTSTRAP_SERVERS}")
print(f"üîó MongoDB: mongo-v4:27017")
print(f"üîó Elasticsearch: elasticsearch-v4:9200")
print()

# T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
os.makedirs(CHECKPOINT_PATH, exist_ok=True)

# Load Sentiment Analysis Model (PhoBERT cho ti·∫øng Vi·ªát)
print("üì¶ Loading Sentiment Analysis Model...")
try:
    sentiment_analyzer = pipeline(
        "sentiment-analysis",
        model="wonrax/phobert-base-vietnamese-sentiment",
        device=0 if torch.cuda.is_available() else -1
    )
    print("‚úÖ Loaded Vietnamese Sentiment Model (PhoBERT)")
except Exception as e:
    print(f"‚ö†Ô∏è  Error loading sentiment model: {e}")
    print("‚ö†Ô∏è  Sentiment analysis will return 'neutral' for all documents")
    sentiment_analyzer = None

print()

# UDF cho Sentiment Analysis
def analyze_sentiment(text):
    """Ph√¢n t√≠ch c·∫£m x√∫c vƒÉn b·∫£n ti·∫øng Vi·ªát"""
    if not text or sentiment_analyzer is None:
        return "neutral"
    
    try:
        # Truncate text ƒë·ªÉ tr√°nh qu√° d√†i
        text_truncated = text[:512] if len(text) > 512 else text
        result = sentiment_analyzer(text_truncated)[0]
        label = result['label'].lower()
        score = result['score']
        
        # Map labels
        if 'pos' in label or 'positive' in label:
            return "positive" if score > 0.6 else "neutral"
        elif 'neg' in label or 'negative' in label:
            return "negative" if score > 0.6 else "neutral"
        else:
            return "neutral"
    except Exception as e:
        print(f"‚ö†Ô∏è  Sentiment error: {e}")
        return "neutral"

sentiment_udf = udf(analyze_sentiment, StringType())

# H√†m train ho·∫∑c load BERTopic model
def get_or_train_bertopic_model(embeddings, documents):
    """Train m·ªõi ho·∫∑c load BERTopic model"""
    
    if os.path.exists(MODEL_PATH):
        print(f"üìÇ Loading existing BERTopic model from {MODEL_PATH}")
        try:
            with open(MODEL_PATH, 'rb') as f:
                topic_model = pickle.load(f)
            
            # Ki·ªÉm tra t√≠nh t∆∞∆°ng th√≠ch c·ªßa model
            test_topic, test_prob = topic_model.transform([documents[0]], embeddings[:1])
            print(f"‚úÖ Model validation: OK (test topic: {test_topic[0]})")
            return topic_model
        except Exception as e:
            print(f"‚ö†Ô∏è  Error loading model: {e}")
            print(f"üîÑ Will retrain model...")
            # X√≥a model c≈© v√† train l·∫°i
            try:
                os.remove(MODEL_PATH)
            except:
                pass
    
    print("üéì No existing model found. Training new BERTopic model...")
    
    if len(embeddings) < MIN_TOPIC_SIZE:
        print(f"‚ö†Ô∏è  Not enough data for training (need at least {MIN_TOPIC_SIZE} documents)")
        return None
    
    try:
        # UMAP ƒë·ªÉ gi·∫£m chi·ªÅu
        umap_model = UMAP(
            n_neighbors=min(15, len(embeddings) - 1),
            n_components=5,
            min_dist=0.0,
            metric='cosine',
            random_state=42
        )
        
        # HDBSCAN cho clustering
        hdbscan_model = HDBSCAN(
            min_cluster_size=MIN_TOPIC_SIZE,
            metric='euclidean',
            cluster_selection_method='eom',
            prediction_data=True
        )
        
        # Kh·ªüi t·∫°o BERTopic
        topic_model = BERTopic(
            embedding_model=None,
            umap_model=umap_model,
            hdbscan_model=hdbscan_model,
            nr_topics=NUM_TOPICS,
            top_n_words=5,
            language='vietnamese',
            calculate_probabilities=True,
            verbose=True
        )
        
        # Train model
        print(f"üéØ Training BERTopic with {len(documents)} documents...")
        topics, probs = topic_model.fit_transform(documents, embeddings)
        
        # L∆∞u model
        os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
        with open(MODEL_PATH, 'wb') as f:
            pickle.dump(topic_model, f)
        
        print(f"‚úÖ Model trained and saved to {MODEL_PATH}")
        print(f"üìä Number of topics: {len(set(topics)) - 1}")  # Tr·ª´ topic -1 (outliers)
        
        return topic_model
        
    except Exception as e:
        print(f"‚ùå Error training BERTopic: {e}")
        import traceback
        traceback.print_exc()
        return None

# Schema cho Kafka message
input_schema = StructType([
    StructField("_id", StringType(), True),
    StructField("title", StringType(), True),
    StructField("content", StringType(), True),
    StructField("url", StringType(), True),
    StructField("source", StringType(), True),
    StructField("category", StringType(), True),
    StructField("published_at", StringType(), True),
    StructField("collected_at", StringType(), True),
    StructField("processed_at", StringType(), True),
    StructField("embedding", ArrayType(DoubleType()), True),
    StructField("embedding_model", StringType(), True),
    StructField("embedding_generated_at", StringType(), True)
])

# ƒê·ªçc d·ªØ li·ªáu t·ª´ Kafka
print(f"üì® Reading from Kafka topic: {KAFKA_INPUT_TOPIC}")

try:
    df_stream = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
        .option("subscribe", KAFKA_INPUT_TOPIC) \
        .option("startingOffsets", "latest") \
        .option("maxOffsetsPerTrigger", 100) \
        .option("failOnDataLoss", "false") \
        .load()
    
    print("‚úÖ Kafka stream initialized")
except Exception as e:
    print(f"‚ùå Error connecting to Kafka: {e}")
    raise

# Parse JSON t·ª´ Kafka
df_parsed = df_stream.select(
    from_json(col("value").cast("string"), input_schema).alias("data")
).select("data.*")

# Th√™m timestamp x·ª≠ l√Ω
df_with_time = df_parsed.withColumn("processing_time", current_timestamp())

# H√†m x·ª≠ l√Ω t·ª´ng micro-batch
def process_batch(batch_df, batch_id):
    """X·ª≠ l√Ω m·ªói micro-batch: BERTopic + sentiment + save"""
    
    print(f"\n{'='*80}")
    print(f"üì¶ Processing Batch #{batch_id} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*80}")
    
    try:
        batch_count = batch_df.count()
        if batch_count == 0:
            print("‚ö†Ô∏è  Empty batch, skipping...")
            return
        
        print(f"üìä Batch size: {batch_count} records")
        
        # Chuy·ªÉn sang Pandas ƒë·ªÉ x·ª≠ l√Ω v·ªõi BERTopic
        batch_pdf = batch_df.toPandas()
        
        # L·ªçc documents c√≥ embedding h·ª£p l·ªá
        batch_pdf = batch_pdf[batch_pdf['embedding'].notna()]
        
        if len(batch_pdf) == 0:
            print("‚ö†Ô∏è  No valid embeddings in batch, skipping...")
            return
        
        print(f"‚úÖ Processing {len(batch_pdf)} documents with valid embeddings")
        
        # Chu·∫©n b·ªã embeddings v√† documents
        embeddings = np.array(batch_pdf['embedding'].tolist())
        documents = batch_pdf['content'].fillna(batch_pdf['title']).tolist()
        
        # Validate embeddings
        print(f"   üìè Embeddings shape: {embeddings.shape}")
        print(f"   üìÑ Documents count: {len(documents)}")
        
        if embeddings.shape[0] != len(documents):
            print("‚ùå Mismatch between embeddings and documents count, skipping...")
            return
        
        # 1. Load ho·∫∑c train BERTopic model
        topic_model = get_or_train_bertopic_model(embeddings, documents)
        
        if topic_model is None:
            print("‚ùå Model not available, skipping batch")
            return
        
        # 2. Topic inference
        try:
            print(f"üéØ Running topic inference...")
            
            # Transform v·ªõi error handling
            topics, probs = topic_model.transform(documents, embeddings)
            
            # X·ª≠ l√Ω probabilities an to√†n
            topic_scores = []
            for i, (t, p) in enumerate(zip(topics, probs)):
                if t >= 0 and t < len(p):
                    topic_scores.append(float(p[t]))
                else:
                    topic_scores.append(0.0)
            
            batch_pdf['topic_id'] = topics
            batch_pdf['topic_score'] = topic_scores
            
            # 3. Extract topic keywords
            topic_keywords_dict = {}
            for topic_id in set(topics):
                if topic_id >= 0:
                    topic_info = topic_model.get_topic(topic_id)
                    if topic_info:
                        keywords = [word for word, _ in topic_info[:5]]
                        topic_keywords_dict[topic_id] = keywords
                    else:
                        topic_keywords_dict[topic_id] = []
                else:
                    topic_keywords_dict[topic_id] = ["outlier"]
            
            batch_pdf['topic_keywords'] = batch_pdf['topic_id'].map(topic_keywords_dict)
            
            print(f"‚úÖ Topic modeling completed")
            
        except Exception as e:
            print(f"‚ùå Error in topic inference: {e}")
            import traceback
            traceback.print_exc()
            return
        
        # 4. Chuy·ªÉn l·∫°i sang Spark DataFrame
        batch_pdf_clean = batch_pdf.drop(columns=['embedding', 'embedding_model', 'embedding_generated_at'])
        
        # Convert topic_keywords t·ª´ list sang string (JSON)
        batch_pdf_clean['topic_keywords'] = batch_pdf_clean['topic_keywords'].apply(
            lambda x: json.dumps(x) if isinstance(x, list) else "[]"
        )
        
        # Define schema
        schema = StructType([
            StructField("_id", StringType(), True),
            StructField("title", StringType(), True),
            StructField("content", StringType(), True),
            StructField("url", StringType(), True),
            StructField("source", StringType(), True),
            StructField("category", StringType(), True),
            StructField("published_at", StringType(), True),
            StructField("collected_at", StringType(), True),
            StructField("processed_at", StringType(), True),
            StructField("processing_time", StringType(), True),
            StructField("topic_id", IntegerType(), True),
            StructField("topic_score", DoubleType(), True),
            StructField("topic_keywords", StringType(), True)
        ])
        
        df_with_topic = spark.createDataFrame(batch_pdf_clean, schema=schema)
        
        # Parse topic_keywords th√†nh array
        df_with_topic = df_with_topic.withColumn(
            "topic_keywords",
            from_json(col("topic_keywords"), ArrayType(StringType()))
        )
        
        # 5. Sentiment Analysis
        print(f"üòä Running sentiment analysis...")
        df_with_sentiment = df_with_topic.withColumn(
            "sentiment",
            sentiment_udf(col("content"))
        )
        
        # 6. Final DataFrame
        df_enriched = df_with_sentiment.select(
            col("_id").alias("doc_id"),
            col("title"),
            col("content"),
            col("published_at"),
            col("source"),
            col("url"),
            col("category"),
            col("topic_id"),
            col("topic_keywords"),
            col("topic_score"),
            col("sentiment"),
            col("processing_time")
        )
        
        # 7. Ghi v√†o Kafka
        try:
            print(f"üì§ Writing to Kafka topic: {KAFKA_OUTPUT_TOPIC}")
            df_kafka_output = df_enriched.select(
                to_json(struct("*")).alias("value")
            )
            
            df_kafka_output.write \
                .format("kafka") \
                .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
                .option("topic", KAFKA_OUTPUT_TOPIC) \
                .save()
            
            print(f"‚úÖ Sent {df_kafka_output.count()} documents to Kafka")
        except Exception as e:
            print(f"‚ö†Ô∏è  Error writing to Kafka: {e}")
        
        # 8. Ghi v√†o Elasticsearch
        try:
            print(f"üì§ Indexing to Elasticsearch...")
            df_es = df_enriched.withColumn("@timestamp", col("processing_time"))
            
            df_es.write \
                .format("org.elasticsearch.spark.sql") \
                .option("es.nodes", "elasticsearch-v4") \
                .option("es.port", "9200") \
                .option("es.resource", "news_enriched/_doc") \
                .option("es.mapping.id", "doc_id") \
                .option("es.nodes.wan.only", "false") \
                .option("es.write.operation", "index") \
                .option("es.batch.size.entries", "1000") \
                .option("es.batch.size.bytes", "10mb") \
                .mode("append") \
                .save()
            
            print(f"‚úÖ Indexed {df_es.count()} documents to Elasticsearch")
            
        except Exception as e:
            print(f"‚ö†Ô∏è  Error writing to Elasticsearch: {e}")
            import traceback
            traceback.print_exc()
        
        # 9. Ghi v√†o MongoDB
        try:
            print(f"üì§ Saving to MongoDB...")
            model_version = datetime.now().strftime("%Y-%m-%d")
            
            df_mongo = df_enriched.select(
                col("doc_id").alias("_id"),
                col("doc_id"),
                col("topic_id"),
                col("topic_score").alias("score"),
                col("sentiment"),
                lit(model_version).alias("model_version"),
                col("processing_time")
            )
            
            df_mongo.write \
                .format("mongo") \
                .mode("append") \
                .save()
            
            print(f"‚úÖ Saved {df_mongo.count()} records to MongoDB")
        except Exception as e:
            print(f"‚ö†Ô∏è  Error writing to MongoDB: {e}")
            import traceback
            traceback.print_exc()
        
        # 10. In th·ªëng k√™
        print(f"\nüìä Batch Statistics:")
        print(f"   Total documents: {df_enriched.count()}")
        
        topic_dist = df_enriched.groupBy("topic_id").count().orderBy(desc("count"))
        print(f"\n   üè∑Ô∏è  Topic Distribution (Top 5):")
        for row in topic_dist.limit(5).collect():
            if row['topic_id'] in topic_keywords_dict:
                keywords_list = topic_keywords_dict[row['topic_id']]
                print(f"      Topic {row['topic_id']}: {row['count']} docs - {keywords_list}")
        
        sentiment_dist = df_enriched.groupBy("sentiment").count().orderBy(desc("count"))
        print(f"\n   üòä Sentiment Distribution:")
        for row in sentiment_dist.collect():
            emoji = {"positive": "üòä", "negative": "üòî", "neutral": "üòê"}.get(row['sentiment'], "")
            print(f"      {emoji} {row['sentiment']}: {row['count']} docs")
        
        print(f"\n{'='*80}\n")
        
    except Exception as e:
        print(f"‚ùå Error processing batch #{batch_id}: {e}")
        import traceback
        traceback.print_exc()

# Ch·∫°y streaming query
print("\n" + "="*80)
print("üöÄ Starting Streaming Query...")
print("="*80)

query = df_with_time \
    .writeStream \
    .foreachBatch(process_batch) \
    .outputMode("append") \
    .option("checkpointLocation", CHECKPOINT_PATH) \
    .trigger(processingTime="30 seconds") \
    .start()

print("\n‚úÖ BERTopic + Sentiment Processor started!")
print(f"   üì® Input: Kafka topic '{KAFKA_INPUT_TOPIC}'")
print(f"   üì§ Output: Kafka '{KAFKA_OUTPUT_TOPIC}', Elasticsearch 'news_enriched', MongoDB 'doc_topics'")
print(f"   ‚è±Ô∏è  Processing interval: 30 seconds")
print(f"   üéØ Model: BERTopic with max {NUM_TOPICS} topics")
print(f"   üòä Sentiment: PhoBERT Vietnamese")
print(f"\nüí° Press Ctrl+C to stop...\n")
print("="*80 + "\n")

# Ch·ªù cho ƒë·∫øn khi d·ª´ng
try:
    query.awaitTermination()
except KeyboardInterrupt:
    print("\n\n" + "="*80)
    print("üõë Stopping streaming query...")
    print("="*80)
    query.stop()
    spark.stop()
    print("‚úÖ Stopped successfully")
    print("="*80)