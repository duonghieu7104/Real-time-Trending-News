from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    col, from_json, current_timestamp, to_json, struct, 
    lit, desc, pandas_udf, expr
)
from pyspark.sql.types import *
import numpy as np
from datetime import datetime
import json
import os
import pickle
from bertopic import BERTopic
from transformers import pipeline
import torch
import pandas as pd
import shutil
from concurrent.futures import ThreadPoolExecutor
import gc

print("="*80)
print("ğŸš€ BERTopic + Sentiment Processor (OPTIMIZED v2 - FIXED)")
print("="*80)

# ==================== Cáº¤U HÃŒNH Tá»I Æ¯U ====================
KAFKA_BOOTSTRAP_SERVERS = "kafka-v4:29092"
KAFKA_INPUT_TOPIC = "processed_data"
KAFKA_OUTPUT_TOPIC = "enriched_news"
MODEL_PATH = "/opt/spark/work-dir/models/bertopic_model.pkl"
CHECKPOINT_PATH = "/opt/spark/work-dir/checkpoints/topic_sentiment"

# Cáº¤U HÃŒNH Tá»I Æ¯U HÃ“A
NUM_TOPICS = 20
MIN_TOPIC_SIZE = 5
BATCH_SIZE = 25  # GIáº¢M Tá»ª 30 XUá»NG 25
TRIGGER_INTERVAL = "180 seconds"  # TÄ‚NG Tá»ª 120s LÃŠN 180s
SENTIMENT_CHUNK_SIZE = 8
MAX_TEXT_LENGTH = 256
MAX_TOPIC_RECORDS = 25  # GIáº¢M Tá»ª 30 XUá»NG 25

RESET_CHECKPOINT = os.getenv("RESET_CHECKPOINT", "false").lower() == "true"

# Reset checkpoint náº¿u cáº§n
if RESET_CHECKPOINT and os.path.exists(CHECKPOINT_PATH):
    print(f"ğŸ”„ Äang reset checkpoint: {CHECKPOINT_PATH}")
    shutil.rmtree(CHECKPOINT_PATH)
    print("âœ… Checkpoint Ä‘Ã£ reset")

os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)
os.makedirs(CHECKPOINT_PATH, exist_ok=True)

# ==================== KHá»I Táº O SPARK Tá»I Æ¯U (ÄÃƒ Sá»¬A Lá»–I) ====================
print("\nğŸ“¦ Khá»Ÿi táº¡o Spark Session...")

spark = SparkSession.builder \
    .appName("BERTopicSentimentOptimizedV2Fixed") \
    .config("spark.jars.packages", 
            "org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.1,"  # âœ… THAY Äá»”I: 3.5.0 â†’ 3.4.1
            "org.mongodb.spark:mongo-spark-connector_2.12:10.2.0,"
            "org.elasticsearch:elasticsearch-spark-30_2.12:8.8.0") \
    .config("spark.mongodb.output.uri", "mongodb://mongo-v4:27017/news_db.doc_topics") \
    .config("spark.es.nodes", "elasticsearch-v4") \
    .config("spark.es.port", "9200") \
    .config("spark.es.resource", "news_enriched") \
    .config("spark.es.nodes.wan.only", "false") \
    .config("spark.broadcast.compress", "true") \
    .config("spark.shuffle.compress", "true") \
    .config("spark.sql.streaming.stopGracefullyOnShutdown", "true") \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.driver.maxResultSize", "2g") \
    .config("spark.sql.execution.arrow.pyspark.enabled", "true") \
    .config("spark.sql.execution.arrow.maxRecordsPerBatch", "3000") \
    .config("spark.default.parallelism", "4") \
    .config("spark.sql.shuffle.partitions", "4") \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.cleaner.periodicGC.interval", "5min") \
    .config("spark.memory.fraction", "0.8") \
    .config("spark.memory.storageFraction", "0.3") \
    .config("spark.executor.cores", "2") \
    .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
    .config("spark.sql.adaptive.skewJoin.enabled", "true") \
    .config("spark.executor.memoryOverhead", "1g") \
    .config("spark.sql.streaming.metricsEnabled", "true") \
    .config("spark.streaming.stopGracefullyOnShutdown", "true") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

print(f"âœ… Spark Ä‘Ã£ khá»Ÿi táº¡o")
print(f"ğŸ“ Model: {MODEL_PATH}")
print(f"ğŸ“ Checkpoint: {CHECKPOINT_PATH}")
print(f"ğŸ”— Kafka: {KAFKA_BOOTSTRAP_SERVERS}")
print(f"ğŸ“¦ Batch size: {BATCH_SIZE}")
print(f"â±ï¸  Trigger interval: {TRIGGER_INTERVAL}")
print()

# ==================== LOAD MODELS ====================

print("ğŸ“¦ Äang táº£i Sentiment Model...")
sentiment_analyzer = None
try:
    sentiment_analyzer = pipeline(
        "sentiment-analysis",
        model="wonrax/phobert-base-vietnamese-sentiment",
        device=0 if torch.cuda.is_available() else -1,
        batch_size=SENTIMENT_CHUNK_SIZE
    )
    print(f"âœ… Sentiment model Ä‘Ã£ táº£i (device: {'GPU' if torch.cuda.is_available() else 'CPU'})")
except Exception as e:
    print(f"âš ï¸  Lá»—i khi táº£i sentiment model: {e}")
    sentiment_analyzer = None

# Load BERTopic
bertopic_model = None
topic_keywords_global = {}

if os.path.exists(MODEL_PATH):
    print(f"ğŸ“‚ Äang táº£i BERTopic model...")
    try:
        with open(MODEL_PATH, 'rb') as f:
            bertopic_model = pickle.load(f)
        
        for topic_id in bertopic_model.get_topics().keys():
            if topic_id >= 0:
                topic_info = bertopic_model.get_topic(topic_id)
                if topic_info:
                    topic_keywords_global[topic_id] = [word for word, _ in topic_info[:5]]
                else:
                    topic_keywords_global[topic_id] = []
            else:
                topic_keywords_global[topic_id] = ["outlier"]
        
        print(f"âœ… BERTopic Ä‘Ã£ táº£i ({len([t for t in topic_keywords_global.keys() if t >= 0])} topics)")
    except Exception as e:
        print(f"âš ï¸  Lá»—i khi táº£i BERTopic: {e}")
        bertopic_model = None
else:
    print("âš ï¸  KhÃ´ng tÃ¬m tháº¥y BERTopic model. Topic modeling bá»‹ táº¯t.")

print()

# ==================== PANDAS UDF Tá»I Æ¯U ====================

@pandas_udf(StringType())
def analyze_sentiment_batch(texts: pd.Series) -> pd.Series:
    """PhÃ¢n tÃ­ch cáº£m xÃºc tá»‘i Æ°u vá»›i batch nhá»"""
    if sentiment_analyzer is None:
        return pd.Series(["neutral"] * len(texts))
    
    results = []
    
    try:
        # Chuáº©n bá»‹ text (Ä‘Ã£ Ä‘Æ°á»£c cáº¯t ngáº¯n tá»« bÃªn ngoÃ i)
        batch_texts = []
        for text in texts:
            if not text or pd.isna(text) or text.strip() == "":
                batch_texts.append("")
            else:
                batch_texts.append(str(text)[:MAX_TEXT_LENGTH])
        
        # Xá»­ lÃ½ theo chunk nhá»
        for i in range(0, len(batch_texts), SENTIMENT_CHUNK_SIZE):
            chunk = batch_texts[i:i+SENTIMENT_CHUNK_SIZE]
            valid_texts = [t for t in chunk if t]
            
            if not valid_texts:
                results.extend(["neutral"] * len(chunk))
                continue
            
            # Inference
            chunk_results = sentiment_analyzer(
                valid_texts, 
                truncation=True, 
                max_length=MAX_TEXT_LENGTH
            )
            
            # Map káº¿t quáº£
            result_idx = 0
            for orig_text in chunk:
                if not orig_text:
                    results.append("neutral")
                else:
                    res = chunk_results[result_idx]
                    label = res['label'].lower()
                    score = res['score']
                    
                    if 'pos' in label and score > 0.6:
                        results.append("positive")
                    elif 'neg' in label and score > 0.6:
                        results.append("negative")
                    else:
                        results.append("neutral")
                    
                    result_idx += 1
                    
    except Exception as e:
        print(f"âš ï¸  Lá»—i sentiment batch: {e}")
        results = ["neutral"] * len(texts)
    
    return pd.Series(results)

# ==================== TOPIC INFERENCE ====================

def infer_topics_batch(embeddings_array, documents_list):
    """Batch topic inference vá»›i error handling"""
    if bertopic_model is None:
        return (
            [-1] * len(documents_list),
            [0.0] * len(documents_list),
            [["no_model"]] * len(documents_list)
        )
    
    try:
        topics, probs = bertopic_model.transform(documents_list, embeddings_array)
        
        topic_scores = []
        for i, (t, p) in enumerate(zip(topics, probs)):
            if t >= 0 and t < len(p):
                topic_scores.append(float(p[t]))
            else:
                topic_scores.append(0.0)
        
        keywords_list = []
        for t in topics:
            keywords_list.append(topic_keywords_global.get(t, ["unknown"]))
        
        return topics, topic_scores, keywords_list
        
    except Exception as e:
        print(f"âš ï¸  Lá»—i topic inference: {e}")
        return (
            [-1] * len(documents_list),
            [0.0] * len(documents_list),
            [["error"]] * len(documents_list)
        )

# ==================== SCHEMA ====================

input_schema = StructType([
    StructField("_id", StringType(), True),
    StructField("title", StringType(), True),
    StructField("content", StringType(), True),
    StructField("url", StringType(), True),
    StructField("source", StringType(), True),
    StructField("category", StringType(), True),
    StructField("published_at", StringType(), True),
    StructField("collected_at", StringType(), True),
    StructField("processed_at", StringType(), True),
    StructField("embedding", ArrayType(DoubleType()), True),
    StructField("embedding_model", StringType(), True),
    StructField("embedding_generated_at", StringType(), True)
])

# ==================== KAFKA STREAM (ÄÃƒ Tá»I Æ¯U) ====================

print(f"ğŸ“¨ Äang káº¿t ná»‘i tá»›i Kafka: {KAFKA_INPUT_TOPIC}")

try:
    df_stream = spark \
        .readStream \
        .format("kafka") \
        .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
        .option("subscribe", KAFKA_INPUT_TOPIC) \
        .option("startingOffsets", "latest") \
        .option("maxOffsetsPerTrigger", BATCH_SIZE) \
        .option("failOnDataLoss", "false") \
        .option("kafkaConsumer.pollTimeoutMs", "180000") \
        .load()
    
    print("âœ… Kafka stream Ä‘Ã£ káº¿t ná»‘i")
except Exception as e:
    print(f"âŒ Káº¿t ná»‘i Kafka tháº¥t báº¡i: {e}")
    raise

# Parse JSON
df_parsed = df_stream.select(
    from_json(col("value").cast("string"), input_schema).alias("data")
).select("data.*")

df_with_time = df_parsed.withColumn("processing_time", current_timestamp())

# ==================== BATCH PROCESSING Tá»I Æ¯U ====================

batch_counter = {"count": 0}

def process_batch(batch_df, batch_id):
    """Xá»­ lÃ½ batch Ä‘Ã£ tá»‘i Æ°u hoÃ n toÃ n"""
    
    batch_counter["count"] += 1
    
    print(f"\n{'='*80}")
    print(f"ğŸ“¦ Batch #{batch_counter['count']} (ID: {batch_id}) - {datetime.now().strftime('%H:%M:%S')}")
    print(f"{'='*80}")
    
    try:
        # PERSIST + COUNT 1 Láº¦N
        batch_df.persist()
        batch_count = batch_df.count()
        
        if batch_count == 0:
            print("âš ï¸  Batch rá»—ng - bá» qua")
            batch_df.unpersist()
            return
        
        print(f"ğŸ“Š Sá»‘ báº£n ghi: {batch_count}")
        
        # Filter vá»›i persist
        df_valid = batch_df.filter(
            col("embedding").isNotNull() & 
            (col("content").isNotNull() | col("title").isNotNull())
        ).persist()
        
        valid_count = df_valid.count()
        
        # Giáº£i phÃ³ng batch_df ngay
        batch_df.unpersist()
        
        if valid_count == 0:
            print("âš ï¸  KhÃ´ng cÃ³ báº£n ghi há»£p lá»‡")
            df_valid.unpersist()
            return
        
        print(f"âœ… Há»£p lá»‡: {valid_count}/{batch_count}")
        
        # ========== TOPIC MODELING (GIá»šI Háº N) ==========
        if bertopic_model is not None:
            print("ğŸ¯ Äang phÃ¢n tÃ­ch topic...")
            
            # CHá»ˆ Láº¤Y Tá»I ÄA MAX_TOPIC_RECORDS
            pdf = df_valid.select("_id", "content", "title", "embedding") \
                .limit(MAX_TOPIC_RECORDS) \
                .toPandas()
            
            embeddings = np.array(pdf['embedding'].tolist())
            # RÃšT NGáº®N TEXT
            documents = pdf['content'].fillna(pdf['title']).str[:500].tolist()
            
            topics, scores, keywords = infer_topics_batch(embeddings, documents)
            
            pdf['topic_id'] = topics
            pdf['topic_score'] = scores
            pdf['topic_keywords'] = keywords
            
            # Back to Spark
            topic_schema = StructType([
                StructField("_id", StringType(), True),
                StructField("topic_id", IntegerType(), True),
                StructField("topic_score", DoubleType(), True),
                StructField("topic_keywords", ArrayType(StringType()), True)
            ])
            
            df_topics = spark.createDataFrame(
                pdf[['_id', 'topic_id', 'topic_score', 'topic_keywords']],
                schema=topic_schema
            )
            
            df_with_topic = df_valid.join(df_topics, on="_id", how="left")
            
            # XÃ“A BIáº¾N Äá»‚ GIáº¢I PHÃ“NG Bá»˜ NHá»š
            del pdf, embeddings, documents, df_topics
            gc.collect()
            
            print("   âœ“ HoÃ n thÃ nh")
        else:
            df_with_topic = df_valid \
                .withColumn("topic_id", lit(-1)) \
                .withColumn("topic_score", lit(0.0)) \
                .withColumn("topic_keywords", expr("array('no_model')"))
        
        # Giáº£i phÃ³ng df_valid
        df_valid.unpersist()
        
        # ========== SENTIMENT (RÃšT NGáº®N TEXT) ==========
        print("ğŸ˜Š Äang phÃ¢n tÃ­ch cáº£m xÃºc...")
        df_with_sentiment = df_with_topic.withColumn(
            "sentiment",
            analyze_sentiment_batch(expr(f"substring(content, 1, {MAX_TEXT_LENGTH})"))
        )
        print("   âœ“ HoÃ n thÃ nh")
        
        # ========== CHUáº¨N Bá»Š OUTPUT ==========
        df_enriched = df_with_sentiment.select(
            col("_id").alias("doc_id"),
            col("title"),
            col("content"),
            col("published_at"),
            col("source"),
            col("url"),
            col("category"),
            col("topic_id"),
            col("topic_keywords"),
            col("topic_score"),
            col("sentiment"),
            col("processing_time")
        ).persist()
        
        output_count = df_enriched.count()
        print(f"ğŸ“¤ Sá»‘ output: {output_count}")
        
        # ========== GHI SONG SONG 3 SINK ==========
        def write_kafka():
            try:
                print("   â†’ Kafka...", end=" ", flush=True)
                
                df_kafka = df_enriched.selectExpr(
                    "CAST(doc_id AS STRING) AS key",
                    "to_json(struct(*)) AS value"
                )
                
                df_kafka.write \
                    .format("kafka") \
                    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
                    .option("topic", KAFKA_OUTPUT_TOPIC) \
                    .option("kafka.acks", "1") \
                    .option("kafka.retries", "3") \
                    .save()
                
                print("âœ“")
            except Exception as e:
                print(f"âœ— ({str(e)[:50]})")
        
        def write_elasticsearch():
            try:
                print("   â†’ Elasticsearch...", end=" ", flush=True)
                
                df_enriched.withColumn("@timestamp", col("processing_time")) \
                    .write \
                    .format("org.elasticsearch.spark.sql") \
                    .option("es.nodes", "elasticsearch-v4") \
                    .option("es.port", "9200") \
                    .option("es.resource", "news_enriched/_doc") \
                    .option("es.mapping.id", "doc_id") \
                    .option("es.batch.size.entries", "500") \
                    .option("es.write.operation", "index") \
                    .mode("append") \
                    .save()
                
                print("âœ“")
            except Exception as e:
                print(f"âœ— ({str(e)[:50]})")
        
        def write_mongodb():
            try:
                print("   â†’ MongoDB...", end=" ", flush=True)
                
                df_enriched.select(
                    col("doc_id").alias("_id"),
                    col("doc_id"),
                    col("topic_id"),
                    col("topic_score").alias("score"),
                    col("sentiment"),
                    lit(datetime.now().strftime("%Y-%m-%d")).alias("model_version"),
                    col("processing_time")
                ).write \
                    .format("mongo") \
                    .mode("append") \
                    .save()
                
                print("âœ“")
            except Exception as e:
                print(f"âœ— ({str(e)[:50]})")
        
        # GHI SONG SONG
        with ThreadPoolExecutor(max_workers=3) as executor:
            futures = [
                executor.submit(write_kafka),
                executor.submit(write_elasticsearch),
                executor.submit(write_mongodb)
            ]
            # Äá»£i táº¥t cáº£ hoÃ n thÃ nh
            for future in futures:
                future.result()
        
        # ========== THá»NG KÃŠ ==========
        print(f"\nğŸ“Š Thá»‘ng kÃª:")
        
        if bertopic_model:
            try:
                top_topics = df_enriched.groupBy("topic_id").count() \
                    .orderBy(desc("count")).limit(3).collect()
                
                print(f"   Topics:")
                for row in top_topics:
                    tid = row['topic_id']
                    if tid >= 0 and tid in topic_keywords_global:
                        kw = ', '.join(topic_keywords_global[tid][:3])
                        print(f"      {tid}: {row['count']} docs ({kw})")
            except:
                pass
        
        try:
            sentiments = df_enriched.groupBy("sentiment").count().collect()
            print(f"   Cáº£m xÃºc: ", end="")
            sentiment_map = {"positive": "ğŸ˜Š", "negative": "ğŸ˜”", "neutral": "ğŸ˜"}
            print(" | ".join([
                f"{sentiment_map.get(r['sentiment'], '')} {r['sentiment']}: {r['count']}" 
                for r in sentiments
            ]))
        except:
            pass
        
        # GIáº¢I PHÃ“NG CACHE
        df_enriched.unpersist()
        gc.collect()
        
        print(f"{'='*80}\n")
        
    except Exception as e:
        print(f"\nâŒ Lá»—i xá»­ lÃ½ batch: {e}")
        import traceback
        traceback.print_exc()
        
        # Cleanup trong trÆ°á»ng há»£p lá»—i
        try:
            batch_df.unpersist()
        except:
            pass
        gc.collect()

# ==================== START STREAMING ====================

print("\n" + "="*80)
print("ğŸš€ Báº®T Äáº¦U STREAMING")
print("="*80)

query = df_with_time \
    .writeStream \
    .foreachBatch(process_batch) \
    .outputMode("append") \
    .option("checkpointLocation", CHECKPOINT_PATH) \
    .trigger(processingTime=TRIGGER_INTERVAL) \
    .start()

print("\nâœ… STREAMING ÄANG HOáº T Äá»˜NG")
print(f"   ğŸ“¨ Input: {KAFKA_INPUT_TOPIC}")
print(f"   ğŸ“¤ Output: {KAFKA_OUTPUT_TOPIC}, Elasticsearch, MongoDB")
print(f"   â±ï¸  Interval: {TRIGGER_INTERVAL} | Batch: {BATCH_SIZE}")
print(f"   ğŸ¯ BERTopic: {'Báº¬T' if bertopic_model else 'Táº®T'} (tá»‘i Ä‘a {MAX_TOPIC_RECORDS} records)")
print(f"   ğŸ˜Š Sentiment: {'Báº¬T' if sentiment_analyzer else 'Táº®T'} (tá»‘i Ä‘a {MAX_TEXT_LENGTH} kÃ½ tá»±)")
print(f"   ğŸ§µ Ghi song song: 3 sinks (Kafka + ES + Mongo)")
print(f"\nğŸ’¡ Tá»‘i Æ°u hÃ³a:")
print(f"   â€¢ Giáº£m batch size: {BATCH_SIZE}")
print(f"   â€¢ TÄƒng trigger interval: {TRIGGER_INTERVAL}")
print(f"   â€¢ Giá»›i háº¡n Ä‘á»™ dÃ i text: {MAX_TEXT_LENGTH} kÃ½ tá»±")
print(f"   â€¢ Ghi song song 3 sink")
print(f"   â€¢ Quáº£n lÃ½ bá»™ nhá»› vá»›i gc.collect()")
print(f"   â€¢ âœ… Sá»¬A Lá»–I: Kafka connector 3.4.1 (tÆ°Æ¡ng thÃ­ch)")
print(f"\nğŸ’¡ Ctrl+C Ä‘á»ƒ dá»«ng | RESET_CHECKPOINT=true Ä‘á»ƒ reset offsets\n")
print("="*80 + "\n")

try:
    query.awaitTermination()
except KeyboardInterrupt:
    print("\n\nğŸ›‘ ÄANG Dá»ªNG...")
    query.stop()
    spark.stop()
    print("âœ… ÄÃƒ Dá»ªNG\n")