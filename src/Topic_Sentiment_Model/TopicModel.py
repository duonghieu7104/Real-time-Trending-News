from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col
from pyspark.sql.types import StructType, StructField, StringType, ArrayType, FloatType
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired
from transformers import pipeline
from elasticsearch import Elasticsearch, helpers
from sklearn.cluster import MiniBatchKMeans
import pandas as pd
import numpy as np
import logging
import os
import pickle
from datetime import datetime
from collections import deque

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# -------------------------------
# Spark Configuration
# -------------------------------
spark = (
    SparkSession.builder
        .appName("Online-Training-BERTopic-Sentiment")
        .config("spark.sql.streaming.metricsEnabled", "true")
        .config("spark.streaming.stopGracefullyOnShutdown", "true")
        .getOrCreate()
)

schema = StructType([
    StructField("_id", StringType(), False),
    StructField("title", StringType(), True),
    StructField("content", StringType(), True),
    StructField("embedding", ArrayType(FloatType()), True),
    StructField("source", StringType(), True),
    StructField("category", StringType(), True),
    StructField("published_at", StringType(), True)
])

# -------------------------------
# Global Variables cho Online Learning
# -------------------------------
MODEL_PATH = "/models/online_bertopic"
UPDATE_FREQUENCY = 100  # C·∫≠p nh·∫≠t model sau m·ªói 100 documents
MAX_BUFFER_SIZE = 1000  # Buffer t·ªëi ƒëa
MIN_DOCS_FOR_TRAINING = 50  # S·ªë docs t·ªëi thi·ªÉu ƒë·ªÉ train l·∫ßn ƒë·∫ßu

# Buffer ƒë·ªÉ t√≠ch l≈©y documents
doc_buffer = deque(maxlen=MAX_BUFFER_SIZE)
embedding_buffer = deque(maxlen=MAX_BUFFER_SIZE)
processed_count = 0
last_update_count = 0

# Load ho·∫∑c kh·ªüi t·∫°o BERTopic model
if os.path.exists(MODEL_PATH):
    logger.info(f"Loading existing model from {MODEL_PATH}")
    topic_model = BERTopic.load(MODEL_PATH)
    model_exists = True
else:
    logger.info("Creating new BERTopic model with MiniBatchKMeans for online learning")
    # S·ª≠ d·ª•ng MiniBatchKMeans ƒë·ªÉ c√≥ th·ªÉ c·∫≠p nh·∫≠t tƒÉng d·∫ßn
    topic_model = BERTopic(
        embedding_model=None,  # D√πng embedding c√≥ s·∫µn
        hdbscan_model=MiniBatchKMeans(n_clusters=20, random_state=42, batch_size=100),
        verbose=False,
        calculate_probabilities=False,  # T·∫Øt ƒë·ªÉ nhanh h∆°n
        nr_topics="auto"
    )
    model_exists = False

# Kh·ªüi t·∫°o Sentiment Pipeline (load 1 l·∫ßn)
logger.info("Loading sentiment analysis model...")
sentiment_pipeline = pipeline(
    "sentiment-analysis",
    model="cardiffnlp/twitter-xlm-roberta-base-sentiment",
    device=-1,  # CPU
    truncation=True,
    max_length=512
)

# Kh·ªüi t·∫°o Elasticsearch client (load 1 l·∫ßn)
logger.info("Connecting to Elasticsearch...")
es_client = Elasticsearch(
    ["http://elasticsearch:9200"],
    retry_on_timeout=True,
    max_retries=3,
    request_timeout=30
)

# Ki·ªÉm tra k·∫øt n·ªëi ES
try:
    res = es_client.info()   # D√πng GET / thay v√¨ HEAD /
    print("‚úÖ Connected to Elasticsearch:", res["cluster_name"])
except Exception as e:
    print("‚ùå Elasticsearch connection failed:", e)

# -------------------------------
# Read from Kafka
# -------------------------------
df = (
    spark.readStream
        .format("kafka")
        .option("kafka.bootstrap.servers", "kafka-v4:29092")
        .option("subscribe", "processed_data")
        .option("startingOffsets", "latest")
        .option("maxOffsetsPerTrigger", "50")  # Batch size nh·ªè h∆°n cho online learning
        .option("failOnDataLoss", "false")
        .load()
)

parsed_df = (
    df.select(from_json(col("value").cast("string"), schema).alias("data"))
      .select("data.*")
)

# -------------------------------
# Batch Processing v·ªõi Online Learning
# -------------------------------
# X·ª≠ l√Ω t·ª´ng batch
batch_results = []

def process_batch(batch_df, batch_id):
    logger.info(f"========== Processing Batch {batch_id} ==========")

    if batch_df.count() == 0:
        logger.info(f"Batch {batch_id} is empty, skipping...")
        return

    try:
        # Chuy·ªÉn sang Pandas
        pdf = batch_df.toPandas()
        logger.info(f"Batch {batch_id}: {len(pdf)} documents")
        
        # Chu·∫©n b·ªã text v√† embeddings
        texts = (pdf["content"].fillna("") + " " + pdf["title"].fillna("")).tolist()
        embeddings = [np.array(emb, dtype=np.float32) for emb in pdf["embedding"].tolist()]
        
        # Th√™m v√†o buffer
        doc_buffer.extend(texts)
        embedding_buffer.extend(embeddings)
        processed_count += len(texts)
        
        logger.info(f"Buffer size: {len(doc_buffer)}/{MAX_BUFFER_SIZE}")
        
        # ===== ONLINE LEARNING LOGIC =====
        
        # TH1: Model ch∆∞a ƒë∆∞·ª£c train l·∫ßn n√†o
        if not model_exists and len(doc_buffer) >= MIN_DOCS_FOR_TRAINING:
            logger.info(f"üî• Initial training with {len(doc_buffer)} documents")
            
            buffer_texts = list(doc_buffer)
            buffer_embeddings = list(embedding_buffer)
            
            topics, probs = topic_model.fit_transform(buffer_texts, buffer_embeddings)
            model_exists = True
            last_update_count = processed_count
            
            # L∆∞u model
            topic_model.save(MODEL_PATH, serialization="pickle", save_ctfidf=True)
            logger.info(f"‚úÖ Model saved to {MODEL_PATH}")
        
        # TH2: Model ƒë√£ t·ªìn t·∫°i, c·∫≠p nh·∫≠t incremental
        elif model_exists and (processed_count - last_update_count) >= UPDATE_FREQUENCY:
            logger.info(f"üîÑ Incremental update after {processed_count - last_update_count} docs")
            
            # L·∫•y documents m·ªõi t·ª´ buffer
            new_docs = list(doc_buffer)[-UPDATE_FREQUENCY:]
            new_embeddings = list(embedding_buffer)[-UPDATE_FREQUENCY:]
            
            # C·∫≠p nh·∫≠t model (partial fit)
            # BERTopic kh√¥ng h·ªó tr·ª£ partial_fit tr·ª±c ti·∫øp, n√™n ta s·∫Ω:
            # 1. D·ª± ƒëo√°n topics cho docs m·ªõi
            # 2. Merge v·ªõi topics c≈©
            # 3. Retrain n·∫øu c·∫ßn
            
            topics, probs = topic_model.transform(new_docs, new_embeddings)
            
            # Ki·ªÉm tra c√≥ topic m·ªõi xu·∫•t hi·ªán kh√¥ng (-1 = outlier)
            unique_topics = set(topics)
            existing_topics = set(topic_model.get_topic_info()["Topic"].tolist())
            
            # N·∫øu c√≥ nhi·ªÅu outliers (>20%), retrain v·ªõi buffer
            outlier_ratio = sum(1 for t in topics if t == -1) / len(topics)
            
            if outlier_ratio > 0.2:
                logger.info(f"‚ö†Ô∏è High outlier ratio ({outlier_ratio:.2%}), retraining...")
                
                buffer_texts = list(doc_buffer)
                buffer_embeddings = list(embedding_buffer)
                
                # Update topics (merge old and new)
                topic_model.update_topics(
                    buffer_texts, 
                    topics=topics,
                    vectorizer_model=topic_model.vectorizer_model
                )
                
                # L∆∞u model ƒë√£ c·∫≠p nh·∫≠t
                topic_model.save(MODEL_PATH, serialization="pickle", save_ctfidf=True)
                logger.info(f"‚úÖ Model updated and saved")
            
            last_update_count = processed_count
        
        # TH3: D·ª± ƒëo√°n v·ªõi model hi·ªán t·∫°i
        if model_exists:
            topics, probs = topic_model.transform(texts, embeddings)
            
            # L·∫•y th√¥ng tin topic
            topic_info = topic_model.get_topics()
            topic_names = []
            topic_keywords = []
            
            for t in topics:
                if t != -1 and t in topic_info:
                    words = [w for w, _ in topic_info[t][:5]]
                    topic_keywords.append(words)
                    topic_names.append(", ".join(words))
                else:
                    topic_keywords.append([])
                    topic_names.append("Outlier/Unknown")
            
            pdf["topic_id"] = topics
            pdf["topic_name"] = topic_names
            pdf["topic_keywords"] = topic_keywords
            pdf["topic_score"] = [float(p) if p is not None else 0.0 for p in probs]
        
        else:
            # Model ch∆∞a s·∫µn s√†ng
            logger.info("‚è≥ Waiting for enough documents to train initial model...")
            pdf["topic_id"] = -1
            pdf["topic_name"] = "Pending"
            pdf["topic_keywords"] = [[]]
            pdf["topic_score"] = 0.0
        
        # ===== SENTIMENT ANALYSIS =====
        logger.info("Analyzing sentiment...")
        sentiments = sentiment_pipeline(texts, truncation=True, max_length=512)
        
        pdf["sentiment_label"] = [s["label"] for s in sentiments]
        pdf["sentiment_score"] = [float(s["score"]) for s in sentiments]
        pdf["processed_at"] = datetime.now().isoformat()
        
        # ===== SAVE TO ELASTICSEARCH =====
        logger.info("Indexing to Elasticsearch...")
        
        # Bulk insert (nhanh h∆°n nhi·ªÅu so v·ªõi t·ª´ng document)
        actions = []
        for _, row in pdf.iterrows():
            doc = row.to_dict()
            
            # Chuy·ªÉn ƒë·ªïi numpy types sang Python native types
            if isinstance(doc.get("topic_keywords"), list):
                doc["topic_keywords"] = [str(k) for k in doc["topic_keywords"]]
            
            action = {
                "_index": "articles_topic_sentiment_online",
                "_id": doc["_id"],
                "_source": doc
            }
            actions.append(action)
        
        # Bulk insert
        success, failed = helpers.bulk(
            es_client, 
            actions, 
            raise_on_error=False,
            stats_only=False
        )
        
        logger.info(f"‚úÖ Batch {batch_id} completed: {success} indexed, {len(failed)} failed")
        
        # Log th·ªëng k√™
        logger.info(f"üìä Stats - Total processed: {processed_count}, Buffer: {len(doc_buffer)}, Model exists: {model_exists}")
        
    except Exception as e:
        logger.error(f"‚ùå Error processing batch {batch_id}: {str(e)}", exc_info=True)


# -------------------------------
# Start Streaming
# -------------------------------
query = (
    parsed_df.writeStream
        .foreachBatch(process_batch)
        .option("checkpointLocation", "/tmp/checkpoints/online_topic_sentiment")
        .trigger(processingTime="30 seconds")
        .start()
)

logger.info("üöÄ Streaming started. Press Ctrl+C to stop.")
query.awaitTermination()