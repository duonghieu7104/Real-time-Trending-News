from kafka import KafkaConsumer
import json
import datetime
import logging
from pymongo import MongoClient, errors

# ========== Logging ==========
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S"
)

# ========== Kafka Config ==========
KAFKA_BROKER = "kafka-v4:29092"
TOPIC = "raw_news"

# ========== MongoDB Config ==========
MONGO_URI = "mongodb://mongo-v4:27017"
DB_NAME = "news_db"
COLLECTION_NAME = "articles"

# K·∫øt n·ªëi MongoDB
mongo_client = MongoClient(MONGO_URI)
db = mongo_client[DB_NAME]
collection = db[COLLECTION_NAME]

# T·∫°o unique index ƒë·ªÉ ch·ªëng tr√πng (theo url)
collection.create_index("url", unique=True)

# ========== Helper Functions ==========
def format_datetime(dt: datetime.datetime):
    return dt.strftime("%d/%m/%Y/%H/%M/%S")

def validate_news_data(data):
    """Validate that the news data has required fields"""
    required_fields = ['source', 'url', 'title', 'content']
    for field in required_fields:
        if field not in data or not data[field]:
            return False, f"Missing or empty field: {field}"
    return True, "Valid"

# ========== Kafka Consumer ==========
consumer = KafkaConsumer(
    TOPIC,
    bootstrap_servers=[KAFKA_BROKER],
    auto_offset_reset="earliest",   # ƒë·ªçc t·ª´ ƒë·∫ßu n·∫øu ch∆∞a c√≥ offset
    enable_auto_commit=True,
    group_id="news-consumer-group",  
    value_deserializer=lambda x: json.loads(x.decode("utf-8")),
    consumer_timeout_ms=30000,  # 30 seconds timeout
    request_timeout_ms=15000,   # 15 seconds request timeout
    session_timeout_ms=10000    # 10 seconds session timeout
)

logging.info("üöÄ B·∫Øt ƒë·∫ßu consume t·ª´ Kafka...")
logging.info(f"üì° Listening to topic: {TOPIC}")
logging.info(f"üóÑÔ∏è MongoDB: {MONGO_URI}/{DB_NAME}/{COLLECTION_NAME}")

try:
    logging.info("üîÑ Starting message consumption loop...")
    message_count = 0
    
    for message in consumer:
        try:
            data = message.value  # dict/json t·ª´ Producer
            message_count += 1
            
            logging.info(f"üì• Received message #{message_count} from {data.get('source', 'unknown')} (partition={message.partition}, offset={message.offset})")
            logging.info(f"üì∞ Title: {data.get('title', 'No title')}")
            
            # Validate data structure
            is_valid, validation_msg = validate_news_data(data)
            if not is_valid:
                logging.warning(f"‚ö†Ô∏è Invalid data skipped: {validation_msg}")
                continue

            # Ghi th·ªùi gian nh·∫≠n ƒë∆∞·ª£c (collected_at) - ch·ªâ n·∫øu ch∆∞a c√≥
            if 'collected_at' not in data or not data['collected_at']:
                data["collected_at"] = format_datetime(datetime.datetime.now())

            # L∆∞u v√†o MongoDB
            try:
                collection.insert_one(data)
                logging.info(f"‚úÖ Inserted into MongoDB: {data['title']} from {data.get('source', 'unknown')}")
            except errors.DuplicateKeyError:
                logging.debug(f"‚ö†Ô∏è Duplicate skipped: {data.get('url')}")
            except Exception as e:
                logging.error(f"‚ùå MongoDB error: {e}")
                
        except Exception as e:
            logging.error(f"‚ùå Error processing message: {e}")
            continue

except KeyboardInterrupt:
    logging.info("üõë Stopping consumer...")
except Exception as e:
    logging.error(f"‚ùå Consumer error: {e}")
finally:
    consumer.close()
    mongo_client.close()
    logging.info("üîå Connections closed")
