from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import subprocess

# ============ Default config cho DAG ============
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2025, 9, 19),   # chá»‰nh láº¡i ngÃ y start náº¿u cáº§n
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}
# ===============================================

# HÃ m gá»i file RSS_Consumer_Continuous.py
def run_rss_consumer():
    import os, sys
    print("ğŸš€ Báº¯t Ä‘áº§u cháº¡y continuous consumer RSS_Consumer_Continuous.py ...")
    print("ğŸ“‚ Current working dir:", os.getcwd())
    print("ğŸ Python executable:", sys.executable)

    result = subprocess.run(
        ["python3", "/opt/airflow/crawler/RSS_Consumer_Continuous.py"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    )
    print("ğŸ“œ STDOUT:\n", result.stdout)
    print("ğŸ“œ STDERR:\n", result.stderr)
    print("ğŸ”¹ Return code:", result.returncode)

    if result.returncode != 0:
        raise Exception(f"Consumer exited with code {result.returncode}")

# Táº¡o DAG
with DAG(
    dag_id="rss_consumer_dag",
    default_args=default_args,
    schedule_interval=None,  # Manual trigger only - will run continuously
    catchup=False,
    tags=["rss", "consumer", "kafka", "mongodb"],
) as dag:

    consume_task = PythonOperator(
        task_id="consume_rss_news",
        python_callable=run_rss_consumer,
    )
