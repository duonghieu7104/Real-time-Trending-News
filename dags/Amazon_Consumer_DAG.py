from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta
import subprocess

# ============ Default config cho DAG ============
default_args = {
    "owner": "airflow",
    "depends_on_past": False,
    "start_date": datetime(2025, 9, 19),   # chá»‰nh láº¡i ngÃ y start náº¿u cáº§n
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}
# ===============================================

# HÃ m gá»i file Tiki_Consumer.py
def run_amazon_consumer():
    import os, sys
    print("ğŸš€ Báº¯t Ä‘áº§u cháº¡y consumer Amazon_Consumer.py ...")
    print("ğŸ“‚ Current working dir:", os.getcwd())
    print("ğŸ Python executable:", sys.executable)

    result = subprocess.run(
        ["python3", "/opt/airflow/crawler/Amazon_Consumer.py"],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    )
    print("ğŸ“œ STDOUT:\n", result.stdout)
    print("ğŸ“œ STDERR:\n", result.stderr)
    print("ğŸ”¹ Return code:", result.returncode)

    if result.returncode != 0:
        raise Exception(f"Consumer exited with code {result.returncode}")

# Táº¡o DAG
with DAG(
    dag_id="amazon_consumer_dag",
    default_args=default_args,
    schedule_interval="0 * * * *",  # cháº¡y má»—i giá», Ä‘á»•i thÃ nh @once náº¿u muá»‘n trigger tay
    catchup=False,
    tags=["amazon", "consumer", "kafka", "minio"],
) as dag:

    consume_task = PythonOperator(
        task_id="consume_amazon_products",
        python_callable=run_amazon_consumer,
    )
